---
title: "Benchmarking Celeritas"
author:
  - "Peter Heywood, Research Software Engineer"
institute:
  - "The University of Sheffield"
date: "2025-04-11"
logo: ./img/tuos/UOSLogo_Primary_Violet_RGB.svg
footer: "Benchmarking Celeritas - [GridPP53 & SWIFT-HEP09](https://indico.cern.ch/event/1476120/)"

format:
  revealjs:
    theme: theme/tuos.scss
    embed-resources: true  # only enable when publish-ready for perf reasons
    template-partials:
      - title-slide.html
    # show-notes: separate-page
    slide-number: c
    width: 1050
    height: 700
    margin: 0.1
    min-scale: 0.2
    max-scale: 2.0
    auto-stretch: false
    fontsize: 32px
    navigation-mode: linear
    controls: true
    mouse-wheel: true
    include-after-body:
      - text: |
          <script type="text/javascript">
          Reveal.addEventListener('slidechanged', (event) => {
            const isSnOn = (event.currentSlide.dataset.hideSlideNumber !== 'true');
            Reveal.configure({ slideNumber: isSnOn ? "c" : isSnOn});
          });
          </script>

# title slide background
title-slide-attributes:
  data-background-color: "#f3f3f3" # $tuos-core-presentation-bg
  # data-background-color: "#9ADBE8" # $tuos-powder-blue

include-in-header:
  # Hide the logo from dark slides for now, rather than figuring out how to switch the logo for the white one
  - text: |
      <style>
        .reveal.has-dark-background .slide-logo {
          display: none !important;
        }
      </style>

---


# Celeritas {.section-header background-image="img/tuos/tuos-core-template-arrows.png" background-size="contain" background-position="right" background-color="#f3f3f3"}
<!-- # Celeritas {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->

## Celeritas

:::: {.columns .smaller}
::: {.column width=85%}

> The Celeritas project implements HEP detector physics on GPU accelerator hardware with the ultimate goal of supporting the massive computational requirements of the HL-LHC upgrade.

<!-- > Celeritas is a Monte Carlo particle transport code for simulating High Energy Physics (HEP) detectors on general purpose GPUs.
> Motivated originally by the massive computational requirements of the High Luminosity upgrade to the Large Hadron Collider,
> the code’s goal is to accelerate the most computationally challenging simulation problems in HEP. -->

:::
::: {.column .text-center width=15%}

![](img/celeritas-project/celeritas-square.svg){fig-alt="Celeritas project Logo"}

:::
::::

:::: {.columns}
::: {.column .smaller width=50%}

<!-- - [github.com/celeritas-project/celeritas](https://github.com/celeritas-project/celeritas) -->
- NVIDIA GPUs via [CUDA](https://developer.nvidia.com/cuda-toolkit)
- AMD GPUs via [HIP](https://github.com/ROCm-Developer-Tools/HIP)
- 2 Geometry implementations
  - [ORANGE](https://celeritas-project.github.io/celeritas/user/implementation/orange.html#api-orange) (CUDA/HIP)
  - [VecGeom](https://gitlab.cern.ch/VecGeom/VecGeom) (CUDA)
- Standalone executables
- Software library

:::
::: {.column width=50%}


::: {.callout-note title="More Information"}
"Accelerating detector simulations with Celeritas: performance improvements and new capabilities"

- [CHEP 2024 presentation](https://indico.cern.ch/event/1338689/contributions/6015932/)
- [arXiv:2503.17608](https://arxiv.org/abs/2503.17608)

<!-- @misc{lund2025acceleratingdetectorsimulationsceleritas,
      title={Accelerating detector simulations with Celeritas: profiling and performance optimizations}, 
      author={Amanda L. Lund and Julien Esseiva and Seth R. Johnson and Elliott Biondo and Philippe Canal and Thomas Evans and Hayden Hollenbeck and Soon Yung Jun and Guilherme Lima and Ben Morgan and Stefano C. Tognini},
      year={2025},
      eprint={2503.17608},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph},
      url={https://arxiv.org/abs/2503.17608}, 
} -->
:::

::: {.text-center .even-smaller}
[github.com/celeritas-project/celeritas](https://github.com/celeritas-project/celeritas)
:::


<!-- ::: {.even-smaller}
> Johnson, Seth R., Amanda Lund, Philippe Canal, Stefano C. Tognini, Julien Esseiva, Soon Yung Jun, Guilherme Lima, et al. 2024. “Celeritas: Accelerating Geant4 with GPUs.” EPJ Web of Conferences 295:11005. [https://doi.org/10.1051/epjconf/202429511005](https://doi.org/10.1051/epjconf/202429511005).
::: -->

:::
::::

## [celeritas-project/regression](https://github.com/celeritas-project/regression)

::: {.smaller}

<!-- > a suite of test problems in Celeritas to track whether 
> the code is able to run to completion without hitting an assertion,
> how the code input options (and processed output) change over time,
> and how the kernel occupancy requirements change in response to growing code complexity -->

> a suite of test problems in Celeritas to track whether
> 
> - the code is able to run to completion without hitting an assertion,
> - how the code input options (and processed output) change over time,
> - how the kernel occupancy requirements change in response to growing code complexity

:::

<!-- [github.com/celeritas-project/regression](https://github.com/celeritas-project/regression) -->

:::: {.columns .smaller}
::: {.column width=55%}

- CPU and GPU runs
- Standalone: `celer-g4`, `celer-sim`
- Library: `geant4` 
- GPU power usage monitoring 
- Node-level benchmarking

:::
::: {.column width=45%}

- ~22 simulation inputs 
  - 7 geometries
  - simulation options (msc, field)
  - orange vs vecgeom

::: {.text-center .smaller}
[github.com/celeritas-project/regression](https://github.com/celeritas-project/regression)
:::

:::
::::

## celeritas-project/regression reference plots

::: {#fig-regression-frontier-perlmutter layout-ncol=2}

![](./img/celeritas-project/regression-v0.5.1-frontier-perlmutter-event-per-node.png){fig-alt="Regression per-node throughput using v0.5.1 on Perlmutter & Frontier"}

![](./img/celeritas-project/regression-v0.5.1-frontier-perlmutter-event-per-energy.png){fig-alt="Regression per-node efficiency using v0.5.1 on Perlmutter & Frontier"}


Per-node (a) throughput and (b) efficiency for Celeritas `v0.5.1` on Frontier & Perlmutter. <br /> Generated using `update-plots.py` from commit [d5b5c03](https://github.com/celeritas-project/regression/tree/d5b5c03773dc47756bc09368ed3b0056e689b12e). <br /> Credit: [celeritas-project/regression contributors](https://github.com/celeritas-project/regression/graphs/contributors)

:::

<!-- # Hardware {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->
# Hardware {.section-header background-image="img/tuos/tuos-core-template-crown.png" background-size="contain" background-position="right" background-color="#f3f3f3"}

<!-- ## Hardware (Upstream)

| Machine                        | CPU per node                    | GPU per node   |
| :----------------------------- | :------------------------------ | :------------- |
| [Frontier][frontier-specs]     | 1x AMD “Optimized 3rd Gen EPYC” | 8x AMD MI250x  |
| [Perlmutter][perlmutter-specs] | 1x AMD EPYC 7763                | 4x NVIDIA A100 |

: {.trbg-nv-amd .table-full-width tbl-colwidths="[19,47,34]"} -->

## Hardware

| Machine                        | CPU per node                    | GPU per node          |
| :----------------------------- | :------------------------------ | :-------------------- |
| [Frontier][frontier-specs]     | 1x AMD “Optimized 3rd Gen EPYC” | 8x AMD MI250x         |
| [Perlmutter][perlmutter-specs] | 1x AMD EPYC 7763                | 4x NVIDIA A100        |
| [JADE 2.5][jade-arc-specs]     | 2x AMD EPYC 9534                | 8x AMD MI300x         |
| [Bede GH200][bede-specs]       | 1x NVIDIA Grace                 | 1x NVIDIA GH200 480GB |
| 3090 (TUoS)                    | 1x Intel i7-5930k               | 2x NVIDIA RTX 3090    |

: {.trbg-nv-amd .shade-r1-r2 .table-full-width tbl-colwidths="[19,47,34]" }

## JADE 2.5 / JADE@ARC

:::: {.columns}
::: {.column .smaller width=60%}

- <u>J</u>oint <u>A</u>ceademic <u>D</u>ata Science <u>E</u>ndeavour 2.5
- UK Tier-2 technology pilot resource
  - Funded by EPSRC
  - Hosted by the University of Oxford
- 3 Lenovo ThinkSystem SR685a V3 Nodes
  - 2 AMD EPYC 9534 64-Core CPUs @ 280W
  - 8 AMD MI300X GPUs @ 750W <!-- need rocsmi to check configured power? -->
- Currently in early accesss / beta phase
  <!-- - i.e. no "real" users yet -->

:::
::: {.column .text-center width=40%}

![](img/oxford-arc/JADE_2.5_Logo.svg){fig-alt="JADE@ARC Logo"}

![<span class="fig-credit">Source: [jade-arc-docs.readthedocs.io](https://jade-arc-docs.readthedocs.io/index.html)</span>](img/oxford-arc/AMD-MI300X.png)


:::
::::

## Bede GH200 Pilot

:::: {.columns}
::: {.column .smaller width=60%}

- N8 CIR Bede Grace-Hopper Pilot
- UK Tier-2 HPC resource
  - Originally funded by EPSRC
  - Hosted by Durham University
  - Extended by N8 partners for 1 year
- 6x NVIDIA GH200 480GB nodes
  - 1 NVIDIA Grace 72-core ARM CPU @ 100W
  - 1 96GB Hopper GPU @ 900W
  - NVLink-C2C host-device interconnect

:::
::: {.column .text-center width=40%}

<style type="text/css">
 .reveal img.gh-fig {
    object-fit:cover; width: 360px; height: 360px;
 }
</style>

![](img/n8-cir-bede/logo-cmyk.png){fig-alt="N8 CIR Bede Logo"}

![NVIDIA Grace Hopper Superchip.<br/><span class="fig-credit">Source: [NVIDIA](https://nvidianews.nvidia.com/news/nvidia-grace-hopper-superchips-designed-for-accelerated-generative-ai-enter-full-production)</span>](img/nvidia/nvidia-gh200-grace-hopper-superchip-platform.png){.gh-fig}

:::
::::

## 2x 3090 Workstation

:::: {.columns}
::: {.column .smaller width=60%}

- Headless Workstation @ TUoS RSE
- 1x Intel i7-5930k (6c 12t) @ 140W 
- 2x NVIDIA RTX 5090 @ 370W
  - Consumer Ampere `SM_86`
  - Limited FP64 hardware (`1:64`)
  - *~Equivalent* to A40 / RTX A5500 / RTX A6000


::: {.callout-important title="Not an ideal benchmark machine"}

- Originally built in ~2015 as a HEDT workstation
- GPUs upgraded in 2021 
- **Biased towards GPUs**
:::

:::
::: {.column .text-center width=40%}

![](img/ptheywood/waimea.jpg){fig-alt="Picture of 2x 3090 workstation" width=400}

:::
::::

<!-- # Benchmarking {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->
# Benchmarking {.section-header background-image="img/tuos/tuos-core-template-rose.png" background-size="contain" background-position="right" background-color="#f3f3f3"}

## Setup `celeritas-project/regression`

::: {.smaller style="margin-top: 1rem;"}

```bash
# Assuming a working install of celeritas
git clone git@github.com:celeritas-project/regression
cd regression
git-lfs install
git-lfs pull
```

:::

::: {.smaller style="margin-top: 1rem"}


```{.python filename="run-problems.py" code-line-numbers="1-8,15"}
class JadeARC(System):
    build_dirs = {
        "orange": Path("/path/to/celeritas/build-ndebug"),
    }
    name = "jadearc"
    num_jobs = 8 # 8 MI300X per node
    gpu_per_job = 1
    cpu_per_job = 16 # 128 core per node
    # ...

# ...

async def main():
  # ...
    _systems = {S.name: S for S in [Frontier, Perlmutter, Wildstyle, JadeARC]}
```

:::

## Setup `celeritas-project/regression` cont.

:::{style="margin-top: 1rem;"}
:::: {.columns .smaller}
::: {.column width=40%}

```{.python filename="analyze.py" code-line-numbers="1,4-5,6-16"}
CPU_POWER_PER_TASK= {
    "frontier": 225 / 8,
    "perlmutter": 280 / 4,
    "jadearc": 280 / 4,
    "bede": 100 / 1,
    "waimea": 140 / 2,
}
GPU_POWER_PER_TASK = {
  # ...
}
CPU_PER_TASK = {
  # ...
}
TASK_PER_NODE = {
  # ...
}
```

:::
::: {.column width=60%}

<style>
  /* Hide x overflow, we dont' care about the offscreen code. */
  .reveal pre.sourceCode.no-hscroll code {
    overflow-x: hidden;
  }
</style>

```{.python filename="update-plots.py" code-line-numbers="1,4-7,12-14" .no-hscroll}
system_color = {
    "frontier": "#BC5544",
    "perlmutter": "#7A954F",
    "jadearc": "#E7298A",
    "bede": "#1B9E77",
    "waimea": "#666666",
}
# ...
def main():
    analyses["frontier"] = plot_minimal("frontier")
    analyses["perlmutter"] = plot_like = plot_all("perlmutter")
    analyses["jadearc"] = plot_minimal("jadearc")
    analyses["bede"] = plot_minimal("bede")
    analyses["waimea"] = plot_minimal("waimea")
    # ...


```
:::
::::
:::

## Running `celeritas-project/regression`

:::{style="margin-top: 1rem;"}

:::: {.columns .smaller}

::: {.column width=100%}

```{.bash filename="run-jadearc.sh" code-line-numbers="5-8,11,14"}
#!/bin/bash -e
#SBATCH -A jade-beta
#SBATCH -p medium
#SBATCH -t 2:59:59
#SBATCH --nodes=1
#SBATCH --ntasks=8
#SBATCH --gres=gpu:8
#SBATCH --cpus-per-gpu=16

# Load modules + activate spack environment
source path/to/jadearc.sh 2> /dev/null

echo "Running on $HOSTNAME at $(date)"
python3 run-problems.py jadearc
echo "Completed at $(date)"
exit 0
```

<!-- ```{.bash}
sbatch run-jadearc.sh
``` -->

:::
::::
:::

## Versions & Limitations

:::: {.columns .smaller}

::: {.column width=50%}

- Celeritas `v0.5.1` 
- CUDA `12.6` 
- ROCm `6.2.1`

:::
::: {.column width=50%}

- 2x 3090
  - No VecGeom results due to Ubuntu/VecGeom link failures
  - Some CPU runs timedout due to old CPU.

:::
::::

:::: {.columns .smaller}

::: {.column width=50%}

- JADE 2.5
  - Power monitoring not implemented (`amd-smi`)
  - Single-GPU run with 16 CPU cores only
    - Per-node results scaled up x8
  - Manually installed dependencies (spack unhappy)

:::
::: {.column width=50%}

- Bede GH200
  - Power monitoring errors
  - G4 GPU offload errors with CUDA OOM
    - 72 CPU cores per GPU is not typical
  - Some G4 failures with OK looking stdout/stderr

:::
::::

<!-- # Results {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->
# Results {.section-header background-image="img/tuos/tuos-core-template-book.png" background-size="contain" background-position="right" background-color="#f3f3f3"}

## Results: Per-node throughput (all)

::: {.text-center}

![All per-node throughput results](img/generated/event-per-node-all.png){height="540"}

:::

## Results: Per-node throughput (AMD)

::: {.text-center}

![Per-node throughput for AMD systems (Frontier & JADE 2.5)<br />Note: JADE@ARC per-node is estimated from a single GPU run.](img/generated/combinations/event-per-node-frontier-jadearc.png){height="540"}

:::

## Results: Per-node throughput (NVIDIA HPC)

::: {.text-center}

![Per-node throughput for NVIDIA HPC systems (Perlmutter & Bede GH200).<br/>GH200 node contains a 72-core CPU & 1 GPU](img/generated/combinations/event-per-node-perlmutter-bede.png){height="540"}

:::

## Results: Per-GPU throughput (NVIDIA HPC)

::: {.text-center}

![Per-**GPU** throughput for NVIDIA HPC systems (Perlmutter & Bede GH200).<br />GH200 node contains a 72-core CPU & 1 GPU](img/generated/combinations/event-per-task-gpu-only-perlmutter-bede.png){height="540"}

:::

## Results: Per-node throughput (AMD & NVIDIA HPC)

::: {.text-center}

![Per-node throughput for AMD & NVIDIA HPC systems (Frontier, Perlmutter, JADE 2.5 & Bede GH200)](img/generated/combinations/event-per-node-frontier-perlmutter-jadearc-bede.png){height="540"}

:::

## Results: Per-GPU throughput (AMD & NVIDIA HPC)

::: {.text-center}

![Per-GPU throughput for AMD & NVIDIA HPC systems (Frontier, Perlmutter, JADE 2.5 & Bede GH200)](img/generated/combinations/event-per-task-gpu-only-frontier-perlmutter-jadearc-bede.png){height="540"}

:::

## Results: Per-node throughput (2x 3090)

::: {.text-center}

![Throughput per node including the 6-core i7 + 2x 3090 workstation.<br />This is an unfair comparison](img/generated/combinations/event-per-node-frontier-perlmutter-jadearc-bede-waimea.png){height="540"}

:::

## Results: Per-GPU throughput (2x 3090)

::: {.text-center}

![Per-GPU throughput for GPU runs only](img/generated/combinations/event-per-task-gpu-only-frontier-perlmutter-jadearc-bede-waimea.png){height="540"}

:::

## Todo / What's next 

- Fix power monitoring/extraction on Jade and Bede
- Full-node mi300x run to check for thermal throttling
- Investigate failures on Bede/GH200
  - OOM, CPU assertions, silent G4 failures
  - Check / update / pin dependencies
- Tidy up into a branch on my fork of `regression` for future re-runs
- Re-run with Celertias 0.6?
- Try Dual 144GB GH200 node when delivered to Bede

# Thank you {.divider .dark-violet visibility="uncounted" data-hide-slide-number='true'}

:::: {.columns}
::: {.column width=27%}
:::
::: {.column .smaller width=44%}

::: {.callout-note title="Acknowledgements"}
- [`celeritas-project/celeritas`](https://github.com/celeritas-project/celeritas/graphs/contributors) developers
- [`celeritas-project/regression`](https://github.com/celeritas-project/regression/graphs/contributors) developers
  - Seth R. Johnson [@sethrj](https://github.com/sethrj)
  - Julien Esseiva [@esseivaju](https://github.com/esseivaju)
  - Amanda Lund [@amandalund](https://github.com/amandalund)
:::

:::
::::

# Additional Slides {.divider .electric-violet visibility="uncounted" data-hide-slide-number='true'}

## Energy efficiency (partial)

::: {.text-center}

![Energy Efficiency for Frontier, Perlmutter and 2x 3090 workstation. <br/>Power consumption extraction not implemented / failed on JADE & Bede](img/generated/event-per-energy.png){height="540"}

:::

<!-- ::: {.callout-important title="Partial power consumption data"}

- `amd-smi` power extraction failed on JADE
- `nvidia-smi` extraction failed on Bede

::: -->

## Results: Per-node throughput (all)

::: {.text-center}

![All per-node throughput results](img/generated/event-per-node-all.png){height="540"}

:::

## Results: Throughput per-GPU w/ G4+GPU

::: {.text-center}

![Per-GPU throughput for GPU and GPU+G4](img/generated/combinations/event-per-task-gpu-g4-frontier-perlmutter-jadearc-bede-waimea.png){height="540"}

:::

<!-- Reference links -->
[perlmutter-specs]: https://docs.nersc.gov/systems/perlmutter/architecture/
[frontier-specs]: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compute-nodes
[bede-specs]: https://bede-documentation.readthedocs.io/en/latest/hardware/index.html
[jade-arc-specs]: https://jade-arc-docs.readthedocs.io/