---
title: "Benchmarking Celeritas"
author:
  - "Peter Heywood, Research Software Engineer"
institute:
  - "The University of Sheffield"
date: "2025-04-11"
logo: ./img/tuos/UOSLogo_Primary_Violet_RGB.svg
footer: "Benchmarking Celeritas - [GridPP53 & SWIFT-HEP09](https://indico.cern.ch/event/1476120/)"

format:
  revealjs:
    theme: theme/tuos.scss
    embed-resources: false  # only enable when publish-ready for perf reasons
    template-partials:
      - title-slide.html
    # show-notes: separate-page
    slide-number: c
    width: 1050
    height: 700
    margin: 0.1
    min-scale: 0.2
    max-scale: 2.0
    auto-stretch: false
    fontsize: 32px
    navigation-mode: linear
    controls: true
    mouse-wheel: true
    include-after-body:
      - text: |
          <script type="text/javascript">
          Reveal.addEventListener('slidechanged', (event) => {
            const isSnOn = (event.currentSlide.dataset.hideSlideNumber !== 'true');
            Reveal.configure({ slideNumber: isSnOn ? "c" : isSnOn});
          });
          </script>

# title slide background
title-slide-attributes:
  data-background-color: "#f3f3f3" # $tuos-core-presentation-bg
  # data-background-color: "#9ADBE8" # $tuos-powder-blue

include-in-header:
  # Hide the logo from dark slides for now, rather than figuring out how to switch the logo for the white one
  - text: |
      <style>
        .reveal.has-dark-background .slide-logo {
          display: none !important;
        }
      </style>

---


# Celeritas {.section-header background-image="img/tuos/tuos-core-template-arrows.png" background-size="contain" background-position="right" background-color="#f3f3f3"}
<!-- # Celeritas {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->

::: {.callout-important appearance="simple" style="width: 50%; margin: 0 auto 0 0;"}
@todo in-slide image attribution
:::


<!-- Only 20 mins. -->

## Celeritas

:::: {.columns .smaller}
::: {.column width=85%}

> The Celeritas project implements HEP detector physics on GPU accelerator hardware with the ultimate goal of supporting the massive computational requirements of the HL-LHC upgrade.

<!-- > Celeritas is a Monte Carlo particle transport code for simulating High Energy Physics (HEP) detectors on general purpose GPUs.
> Motivated originally by the massive computational requirements of the High Luminosity upgrade to the Large Hadron Collider,
> the code’s goal is to accelerate the most computationally challenging simulation problems in HEP. -->

:::
::: {.column .text-center width=15%}

![](img/celeritas-project/celeritas-square.svg){fig-alt="Celeritas project Logo"}

:::
::::

:::: {.columns}
::: {.column .smaller width=50%}

<!-- - [github.com/celeritas-project/celeritas](https://github.com/celeritas-project/celeritas) -->
- NVIDIA GPUs via [CUDA](https://developer.nvidia.com/cuda-toolkit)
- AMD GPUs via [HIP](https://github.com/ROCm-Developer-Tools/HIP)
- 2 Geometry implementations
  - [ORANGE](https://celeritas-project.github.io/celeritas/user/implementation/orange.html#api-orange) (CUDA/HIP)
  - [VecGeom](https://gitlab.cern.ch/VecGeom/VecGeom) (CUDA)
- Standalone executables
- Software library

:::
::: {.column width=50%}


::: {.callout-note title="More Information"}
"Accelerating detector simulations with Celeritas: performance improvements and new capabilities"

- [CHEP 2024 presentation](https://indico.cern.ch/event/1338689/contributions/6015932/)
- [arXiv:2503.17608](https://arxiv.org/abs/2503.17608)

<!-- @misc{lund2025acceleratingdetectorsimulationsceleritas,
      title={Accelerating detector simulations with Celeritas: profiling and performance optimizations}, 
      author={Amanda L. Lund and Julien Esseiva and Seth R. Johnson and Elliott Biondo and Philippe Canal and Thomas Evans and Hayden Hollenbeck and Soon Yung Jun and Guilherme Lima and Ben Morgan and Stefano C. Tognini},
      year={2025},
      eprint={2503.17608},
      archivePrefix={arXiv},
      primaryClass={physics.comp-ph},
      url={https://arxiv.org/abs/2503.17608}, 
} -->
:::

::: {.text-center .even-smaller}
[github.com/celeritas-project/celeritas](https://github.com/celeritas-project/celeritas)
:::


<!-- ::: {.even-smaller}
> Johnson, Seth R., Amanda Lund, Philippe Canal, Stefano C. Tognini, Julien Esseiva, Soon Yung Jun, Guilherme Lima, et al. 2024. “Celeritas: Accelerating Geant4 with GPUs.” EPJ Web of Conferences 295:11005. [https://doi.org/10.1051/epjconf/202429511005](https://doi.org/10.1051/epjconf/202429511005).
::: -->

:::
::::

## [celeritas-project/regression](https://github.com/celeritas-project/regression)

::: {.smaller}

<!-- > a suite of test problems in Celeritas to track whether 
> the code is able to run to completion without hitting an assertion,
> how the code input options (and processed output) change over time,
> and how the kernel occupancy requirements change in response to growing code complexity -->

> a suite of test problems in Celeritas to track whether
> 
> - the code is able to run to completion without hitting an assertion,
> - how the code input options (and processed output) change over time,
> - how the kernel occupancy requirements change in response to growing code complexity

:::

<!-- [github.com/celeritas-project/regression](https://github.com/celeritas-project/regression) -->

:::: {.columns .smaller}
::: {.column width=55%}

- CPU and GPU runs
- Standalone: `celer-g4`, `celer-sim`
- Library: `geant4` 
- GPU power usage monitoring 
- Node-level benchmarking

:::
::: {.column width=45%}

- ~22 simulation inputs 
  - 7 geometries
  - orange vs vecgeom
  - simulation options (msc, field)

::: {.text-center .smaller}
[github.com/celeritas-project/regression](https://github.com/celeritas-project/regression)
:::

:::
::::

## celeritas-project/regression

::: {.callout-important appearance="simple"}
@todo - image credit
:::


::: {#fig-regression-frontier-perlmutter layout-ncol=2}

![](./img/celeritas-project/regression-v0.5.1-frontier-perlmutter-event-per-node.png){fig-alt="Regression per-node throughput using v0.5.1 on Perlmutter & Frontier"}

![](./img/celeritas-project/regression-v0.5.1-frontier-perlmutter-event-per-energy.png){fig-alt="Regression per-node efficiency using v0.5.1 on Perlmutter & Frontier"}


Per-node (a) throughput and (b) efficiency for Celeritas v0.5.1 on Perlmutter & Frontier. Credit @todo

:::



<!-- # Hardware {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->
# Hardware {.section-header background-image="img/tuos/tuos-core-template-crown.png" background-size="contain" background-position="right" background-color="#f3f3f3"}



## Hardware (Upstream)

| Machine                        | CPU                             | GPU            |
| :----------------------------- | :------------------------------ | :------------- |
| [Frontier][frontier-specs]     | 1x AMD “Optimized 3rd Gen EPYC” | 8x AMD MI250x  |
| [Perlmutter][perlmutter-specs] | 1x AMD EPYC 7763                | 4x NVIDIA A100 |

: {.trbg-nv-amd .table-full-width tbl-colwidths="[19,47,34]"}

## JADE 2.5 / JADE@ARC

:::: {.columns}
::: {.column .smaller width=60%}

- <u>J</u>oint <u>A</u>ceademic <u>D</u>ata Science <u>E</u>ndeavour 2.5
- UK Tier-2 technology pilot resource
  - Funded by EPSRC
  - Hosted by the University of Oxford
- 3 Lenovo ThinkSystem SR685a V3 Nodes
  - 2 AMD EPYC 9534 64-Core CPUs
  - 8 AMD MI300X GPUs
- Currently in a beta phase
  - i.e. no "real" users yet

:::
::: {.column .text-center width=40%}

![](img/oxford-arc/JADE_2.5_Logo.svg){fig-alt="JADE@ARC Logo"}
![](img/oxford-arc/AMD-MI300X.png){fig-alt="AMD MI300x system image"}

:::
::::

## Bede GH200 Pilot

:::: {.columns}
::: {.column .smaller width=60%}

- N8 CIR Bede Grace-Hopper Pilot
- UK Tier-2 HPC resource
  - Originally funded by EPSRC
  - Hosted by Durham University
  - Extended by N8 partners for 1 year
- 6x NVIDIA GH200 480GB nodes
  - 1 NVIDIA Grace 72-core ARM CPU
  - 1 96GB Hopper GPU
  - NVLink-C2C host-device interconnect

:::
::: {.column .text-center width=40%}

<style type="text/css">
 .reveal img.gh-fig {
    object-fit:cover; width: 400px; height: 400px;
 }
</style>

![](img/n8-cir-bede/logo-cmyk.png){fig-alt="N8 CIR Bede Logo"}
![NVIDIA Grace Hopper Superchip](img/nvidia/nvidia-gh200-grace-hopper-superchip-platform.png){.gh-fig}

:::
::::

## 2x 3090 Workstation

:::: {.columns}
::: {.column .smaller width=60%}

- Headless Workstation @ TUoS RSE
- 1x Intel i7-5930k (6c 12t) @ 140W 
- 2x NVIDIA RTX 5090 @ 370W
  - Consumer Ampere `SM_86`
  - Limited FP64 hardware (`1:64`)
  - *~Equivalent* to A40 / RTX A5500 / RTX A6000


::: {.callout-important title="Not an ideal benchmark machine"}

- Originally built in ~2015 as a HEDT workstation
- GPUs upgraded in 2021 
- **Biased towards GPUs**
:::

:::
::: {.column .text-center width=40%}



![](img/ptheywood/waimea.jpg){fig-alt="Picture of Waimea during GPU upgrade" width=400}


:::
::::

## Hardware

| Machine                        | CPU                             | GPU                   |
| :----------------------------- | :------------------------------ | :-------------------- |
| [Frontier][frontier-specs]     | 1x AMD “Optimized 3rd Gen EPYC” | 8x AMD MI250x         |
| [Perlmutter][perlmutter-specs] | 1x AMD EPYC 7763                | 4x NVIDIA A100        |
| [JADE 2.5][jade-arc-specs]     | 2x AMD EPYC 9534                | 8x AMD MI300x         |
| [Bede GH200][bede-specs]       | 1x NVIDIA Grace                 | 1x NVIDIA GH200 480GB |
| 3090 (TUoS)                    | 1x Intel i7-5930k               | 2x NVIDIA RTX 3090    |

: {.trbg-nv-amd .shade-r1-r2 .table-full-width tbl-colwidths="[19,47,34]" }




<!-- # Benchmarking {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->
# Benchmarking {.section-header background-image="img/tuos/tuos-core-template-rose.png" background-size="contain" background-position="right" background-color="#f3f3f3"}

## Setup `celeritas-project/regression`

::: {.smaller style="margin-top: 1rem;"}

```bash
# Assuming a working install of celeritas
git clone git@github.com:celeritas-project/regression
cd regression
git-lfs install
git-lfs pull
```

:::

::: {.smaller style="margin-top: 1rem"}


```{.python filename="run-problems.py" code-line-numbers="1-8,15"}
class JadeARC(System):
    build_dirs = {
        "orange": Path("/path/to/celeritas/build-ndebug"),
    }
    name = "jadearc"
    num_jobs = 1 # 8 MI300X per node
    gpu_per_job = 1
    cpu_per_job = 16 # 128 core per node
    # ...

# ...

async def main():
  # ...
    _systems = {S.name: S for S in [Frontier, Perlmutter, Wildstyle, JadeARC]}
```

:::

## Update post-processing `celeritas-project/regression`

::: {.callout-important appearance="minimal"}
@todo - Update this when I have the actual plotting script done
:::

:::{style="margin-top: 1rem;"}
:::: {.columns .smaller}
::: {.column width=40%}

```{.python filename="analyze.py" code-line-numbers="1,4-5,6-14"}
CPU_POWER_PER_TASK= {
    "frontier": 225 / 8,
    "perlmutter": 280 / 4,
    "jadearc": 280 / 4,
}
GPU_POWER_PER_TASK = {
  # ...
}
CPU_PER_TASK = {
  # ...
}
TASK_PER_NODE = {
  # ...
}
```

:::
::: {.column width=60%}

```{.python filename="update-plots.py" code-line-numbers="1,4-7,10-11"}
system_color = {
    "frontier": "#BC5544",
    "perlmutter": "#7A954F",
    "jadearc": "#d95f02",
}
# ...
def main():
    analyses["frontier"] = plot_minimal("frontier")
    analyses["perlmutter"] = plot_like = plot_all("perlmutter")
    analyses["jadearc"] = plot_minimal("jadearc")
    # ...
```
:::
::::
:::

## Running `celeritas-project/regression`

:::{style="margin-top: 1rem;"}

:::: {.columns .smaller}

::: {.column width=100%}

```{.bash filename="run-jadearc.sh" code-line-numbers="5-8,11,14"}
#!/bin/bash -e
#SBATCH -A jade-beta
#SBATCH -p medium
#SBATCH -t 2:59:59
#SBATCH --nodes=1
#SBATCH --ntasks=8
#SBATCH --gres=gpu:8
#SBATCH --cpus-per-gpu=16

# Load modules + activate spack environment
source path/to/jadearc.sh 2> /dev/null

echo "Running on $HOSTNAME at $(date)"
python3 run-problems.py jadearc
echo "Completed at $(date)"
exit 0
```

<!-- ```{.bash}
sbatch run-jadearc.sh
``` -->

:::
::::
:::

## @todo better title - Partial / Incomplete results

- Celeritas `v0.5.1` using CUDA `12.6` / ROCm `6.2.1`

- JADE 2.5
  - Single-GPU run only, due to queue time for full node? 
  - **@todo check this.**
  - Manually installed dependencies, Spack was unhappy with system provided compilers
- Bede GH200
  - `72` host threads + `1` GPU led to CUDA OOM - reduced to `36`
  - **Some errors @todo.**
- 3090
  - No VecGeom results due to Ubuntu/VecGeom link failures


<!-- # Results {.divider .pale-violet visibility="uncounted" data-hide-slide-number='true'} -->
# Results {.section-header background-image="img/tuos/tuos-core-template-book.png" background-size="contain" background-position="right" background-color="#f3f3f3"}

## Results Frontier vs JADE 2.5? @todo

::: {.callout-important title="@todo"}

- Figure / table for some results / comparisons, depending on what plotting works nicely.

:::


## Results Perlmutter vs GH200? @todo

::: {.callout-important title="@todo"}

- Figure / table for some results / comparisons, depending on what plotting works nicely.

:::

## Results Perlmutter vs GH200 vs 3090? @todo

::: {.callout-important title="@todo"}

- Figure / table for some results / comparisons, depending on what plotting works nicely.

:::

## Results Everything? @todo

::: {.callout-important title="@todo"}

- Figure / table for some results / comparisons, depending on what plotting works nicely.

:::

## Results Per GPU? @todo


::: {.callout-important title="@todo"}

- Figure / table for some results / comparisons, depending on what plotting works nicely.

:::

## Results - Power consumation? @todo

::: {.callout-important title="@todo"}

- Figure / table for some results / comparisons, depending on what plotting works nicely.

:::

## Todo / What's next 

::: {.callout-important title="@todo"}
Finish this slide?
:::

- Finish/Fix `roc-smi` power monitoring
- Re-run on a full 8x MI300x node (in-queue)
- Investigate failures on GH200 and re-run
  - Try Celeritas v0.6?
  - Update Spack for newer dependency resolution?
- Tidy up into a branch on my fork of `regression` for future re-runs


# Thank you {.divider .dark-violet visibility="uncounted" data-hide-slide-number='true'}

:::: {.columns}
::: {.column width=27%}
:::
::: {.column .smaller width=44%}

::: {.callout-tip title="Acknowledgements"}
- [`celeritas-project/celeritas`](https://github.com/celeritas-project/celeritas/graphs/contributors) developers
- [`celeritas-project/regression`](https://github.com/celeritas-project/regression/graphs/contributors) developers
  - Seth R. Johnson [@sethrj](https://github.com/sethrj)
  - Julien Esseiva [@esseivaju](https://github.com/esseivaju)
  - Amanda Lund [@amandalund](https://github.com/amandalund)
:::

:::
::::

<!-- # Additional Slides {.divider .electric-violet visibility="uncounted" data-hide-slide-number='true'} -->

<!-- Reference links -->
[perlmutter-specs]: https://docs.nersc.gov/systems/perlmutter/architecture/
[frontier-specs]: https://docs.olcf.ornl.gov/systems/frontier_user_guide.html#frontier-compute-nodes
[bede-specs]: https://bede-documentation.readthedocs.io/en/latest/hardware/index.html
[jade-arc-specs]: https://jade-arc-docs.readthedocs.io/